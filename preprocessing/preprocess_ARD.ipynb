{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:19:39.422963Z",
     "start_time": "2020-01-09T01:19:39.417422Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:19:59.036142Z",
     "start_time": "2020-01-09T01:19:39.435378Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.logger import PrettyLogger\n",
    "from utils.date import str2date, int2date_delta, date2str\n",
    "from utils.io_func import save_to_npy, load_from_tiff\n",
    "from config import (\n",
    "    START_V_I, START_H_I, SIDE_LEN, INTRPL_START_DATE_STR, INTRPL_END_DATE_STR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:19:59.043143Z",
     "start_time": "2020-01-09T01:19:59.039422Z"
    }
   },
   "outputs": [],
   "source": [
    "logger = PrettyLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:19:59.055840Z",
     "start_time": "2020-01-09T01:19:59.045850Z"
    }
   },
   "outputs": [],
   "source": [
    "SITE = \"Site_A\"\n",
    "YEAR = \"2015\"\n",
    "DATA_DIR = \"../data/{}/ARD/{}/\".format(SITE, YEAR)\n",
    "OUTPUT_DIR = \"./out/{}/ARD/cropped_interpolated/{}/\".format(SITE, YEAR)\n",
    "AVAI_PATH = os.path.join(OUTPUT_DIR, \"availability.npy\")\n",
    "FILTER_BAND_PATH = os.path.join(OUTPUT_DIR, \"filter_band.npy\")\n",
    "INTERPOLATED_PATH = os.path.join(OUTPUT_DIR, \"interpolated.npy\")\n",
    "FINAL_OUTOUT_FILEPATH = \"./out/{}/x-{}.npy\".format(SITE, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARD observation input, cropping and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:19:59.109273Z",
     "start_time": "2020-01-09T01:19:59.059000Z"
    }
   },
   "outputs": [],
   "source": [
    "# link the filenames to date\n",
    "date_filename_dict = {}\n",
    "for filename in sorted(os.listdir(DATA_DIR)):\n",
    "    date = str2date(filename[15:23])\n",
    "    if (\n",
    "        date >= str2date(\"{}{}\".format(YEAR, INTRPL_START_DATE_STR))\n",
    "        and date <= str2date(\"{}{}\".format(YEAR, INTRPL_END_DATE_STR))\n",
    "    ):\n",
    "        if date not in date_filename_dict.keys():\n",
    "            date_filename_dict[date] = []\n",
    "        date_filename_dict[date].append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:36:33.301768Z",
     "start_time": "2020-01-09T01:19:59.112110Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read ARD images, crop ARD images and detect invalid values\n",
    "raw_dates = sorted(date_filename_dict.keys())\n",
    "availability = np.zeros((SIDE_LEN, SIDE_LEN, len(raw_dates)))\n",
    "valid = np.zeros((SIDE_LEN, SIDE_LEN, len(raw_dates), 6))\n",
    "\n",
    "to_fill = np.vectorize(lambda x: int(\"{:011b}\".format(x)[-1], 2))\n",
    "to_clear = np.vectorize(lambda x: int(\"{:011b}\".format(x)[-2], 2))\n",
    "to_cloud_shadow = np.vectorize(lambda x: int(\"{:011b}\".format(x)[-4], 2))\n",
    "to_cloud = np.vectorize(lambda x: int(\"{:011b}\".format(x)[-6], 2))\n",
    "for i, date in enumerate(raw_dates):\n",
    "    logger.info(\"Loading: {}/{}\".format(i+1, len(raw_dates)), date2str(date))\n",
    "    sr_bands = []\n",
    "    for filename in date_filename_dict[date]:\n",
    "        band = load_from_tiff(os.path.join(DATA_DIR, filename))[\n",
    "            START_V_I:START_V_I+SIDE_LEN, START_H_I:START_H_I+SIDE_LEN\n",
    "        ]\n",
    "        if filename[-11:-4] != \"PIXELQA\":\n",
    "            sr_bands.append(band)\n",
    "        else:\n",
    "            qa_band = band\n",
    "    sr_bands = np.array(sr_bands).transpose((1, 2, 0))\n",
    "\n",
    "    flag_sr_range = ((sr_bands >= 0) & (sr_bands <= 10000)).all(axis=2)\n",
    "    fill_band = to_fill(qa_band)\n",
    "    flag_fill = (fill_band == 0)\n",
    "    clear_band = to_clear(qa_band)\n",
    "    flag_clear = (clear_band == 1)\n",
    "    cloud_shadow_band = to_cloud_shadow(qa_band)\n",
    "    flag_cloud_shadow = (cloud_shadow_band == 0)\n",
    "    cloud_band = to_cloud(qa_band)\n",
    "    flag_cloud = (cloud_band == 0)\n",
    "    flag = flag_sr_range*flag_fill*flag_clear*flag_cloud_shadow*flag_cloud\n",
    "\n",
    "    availability[:, :, i] = flag\n",
    "\n",
    "    # make invalid observations zero, only for the convenience of debugging\n",
    "    valid[:, :, i, :] = sr_bands\n",
    "    valid[:, :, i, :] = valid[:, :, i, :]*(flag.reshape(*flag.shape, 1))\n",
    "\n",
    "save_to_npy(availability, AVAI_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:36:34.384851Z",
     "start_time": "2020-01-09T01:36:33.306697Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========== PIXEL FILTER METHOD BY AVAILABILITY ==========\n",
    "If the number of valid observations after May 15 >= 7,\n",
    "the pixel will be included in the dataset, otherwise it will be excluded.\n",
    "\"\"\"\n",
    "\n",
    "index4filter = raw_dates.index(list(filter(\n",
    "    lambda x: x > str2date(\"{}0515\".format(YEAR)), raw_dates\n",
    "))[0])\n",
    "filter_band = availability[:, :, index4filter:].sum(axis=2) >= 7\n",
    "logger.info(\"Validity percentage ({} {}): {:.4f}\".format(\n",
    "    SITE, YEAR,\n",
    "    filter_band.sum()/(filter_band.shape[0]*filter_band.shape[1])\n",
    "))\n",
    "save_to_npy(filter_band, FILTER_BAND_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T01:36:38.032317Z",
     "start_time": "2020-01-09T01:36:36.930790Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare target dates for interpolation\n",
    "intrpl_start_date = str2date(\"{}{}\".format(YEAR, INTRPL_START_DATE_STR))\n",
    "intrpl_end_date = str2date(\"{}{}\".format(YEAR, INTRPL_END_DATE_STR))\n",
    "intrpl_delta_days = list(range(\n",
    "    0, (intrpl_end_date - intrpl_start_date).days + 1, 7\n",
    "))\n",
    "intrpl_dates = [\n",
    "    int2date_delta(intrpl_delta_day) + intrpl_start_date\n",
    "    for intrpl_delta_day in intrpl_delta_days\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T03:00:21.154986Z",
     "start_time": "2020-01-09T01:36:38.035281Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "========== INTERPOLATION METHOD ==========\n",
    "situation I (normal): d_1, d_2*, target, d_3*, d_4, ...\n",
    "situation II (close to the start date): target, d_1*, d_2*, d_3, ...\n",
    "situation III (close to the end date): d_1, d_2, ..., d_(-2), d_(-1), target\n",
    "'''\n",
    "\n",
    "interpolated = np.zeros((SIDE_LEN, SIDE_LEN, len(intrpl_dates), 6))\n",
    "for intrpl_date_index, intrpl_date in enumerate(intrpl_dates):\n",
    "    logger.info(\"Interpolating: {}/{} {} \".format(\n",
    "        intrpl_date_index + 1, len(intrpl_dates), date2str(intrpl_date))\n",
    "    )\n",
    "    # descending/ascending order for searching the nearest day before/after\n",
    "    before_dates = list(filter(lambda x: x <= intrpl_date, raw_dates))[::-1]\n",
    "    after_dates = list(filter(lambda x: x >= intrpl_date, raw_dates))\n",
    "\n",
    "    for i in range(SIDE_LEN):\n",
    "        for j in range(SIDE_LEN):\n",
    "\n",
    "            # filter invalid pixel\n",
    "            if not filter_band[i, j]:\n",
    "                continue\n",
    "\n",
    "            # situation I\n",
    "            d_1 = None\n",
    "            for nearest_before_index, before_date in enumerate(before_dates):\n",
    "                before_date_raw_index = raw_dates.index(before_date)\n",
    "                if availability[i, j][before_date_raw_index]:\n",
    "                    d_1 = before_date\n",
    "                    date_raw_index_1 = before_date_raw_index\n",
    "                    break\n",
    "            d_2 = None\n",
    "            for nearest_after_index, after_date in enumerate(after_dates):\n",
    "                after_date_raw_index = raw_dates.index(after_date)\n",
    "                if availability[i, j][after_date_raw_index]:\n",
    "                    d_2 = after_date\n",
    "                    date_raw_index_2 = after_date_raw_index\n",
    "                    break\n",
    "\n",
    "            # situation II: search the second nearest after date\n",
    "            if not d_1:\n",
    "                for after_date in after_dates[nearest_after_index+1:]:\n",
    "                    after_date_raw_index = raw_dates.index(after_date)\n",
    "                    if availability[i, j][after_date_raw_index]:\n",
    "                        d_1 = after_date\n",
    "                        date_raw_index_1 = after_date_raw_index\n",
    "                        break\n",
    "\n",
    "            # situation III: search the second nearest before date\n",
    "            if not d_2:\n",
    "                for before_date in before_dates[nearest_before_index+1:]:\n",
    "                    before_date_raw_index = raw_dates.index(before_date)\n",
    "                    if availability[i, j][before_date_raw_index]:\n",
    "                        d_2 = before_date\n",
    "                        date_raw_index_2 = before_date_raw_index\n",
    "                        break\n",
    "\n",
    "            interpolated[i][j][intrpl_date_index] = [np.interp(\n",
    "                (intrpl_date-d_1).days,\n",
    "                [0, (d_2-d_1).days],\n",
    "                [valid[i, j, date_raw_index_1, band_index],\n",
    "                    valid[i, j, date_raw_index_2, band_index]]\n",
    "            ) for band_index in range(6)]\n",
    "\n",
    "save_to_npy(interpolated, INTERPOLATED_PATH)\n",
    "x = interpolated[filter_band]\n",
    "save_to_npy(x, FINAL_OUTOUT_FILEPATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
